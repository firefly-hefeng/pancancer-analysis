#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GEO Data Smart Re-verification Module
æ™ºèƒ½å¤æ ¸æ¨¡å— - ä¼˜å…ˆçº§åˆ†çº§ï¼Œå‡å°‘APIè°ƒç”¨
Version 2.0 - Cost Optimized
"""

import os
import time
import pandas as pd
import json
from datetime import datetime
from typing import Dict, List, Tuple
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed  # æ·»åŠ è¿™ä¸€è¡Œ
class DataReVerifier:
    """æ•°æ®å¤æ ¸å™¨ - ä½¿ç”¨Kimi APIå¯¹å¯ç–‘æ•°æ®è¿›è¡ŒäºŒæ¬¡ç¡®è®¤"""
    
    def __init__(self, kimi_client, series_df: pd.DataFrame, 
                 confidence_threshold_high: float = 0.85,
                 confidence_threshold_low: float = 0.4):
        """
        Parameters:
        -----------
        kimi_client : KimiAPIClient
            Kimi APIå®¢æˆ·ç«¯
        series_df : pd.DataFrame
            Serieså…ƒæ•°æ®
        confidence_threshold_high : float
            é«˜ç½®ä¿¡åº¦é˜ˆå€¼ - è¶…è¿‡æ­¤å€¼ç›´æ¥åˆ¤å®š
        confidence_threshold_low : float
            ä½ç½®ä¿¡åº¦é˜ˆå€¼ - ä½äºæ­¤å€¼ç›´æ¥æ’é™¤
        """
        self.kimi_client = kimi_client
        self.series_df = series_df
        self.confidence_threshold_high = confidence_threshold_high
        self.confidence_threshold_low = confidence_threshold_low
        
        # ç»“æœå­˜å‚¨
        self.confirmed_clean = []      # ç¡®è®¤ä¸ºæ¸…æ´æ•°æ®
        self.confirmed_removed = []    # ç¡®è®¤åº”ç§»é™¤
        self.need_manual_review = []   # éœ€è¦äººå·¥å¤å®¡
    
    def _safe_str(self, value) -> str:
        """å®‰å…¨å­—ç¬¦ä¸²è½¬æ¢"""
        if pd.isna(value) or value is None:
            return ''
        return str(value)
    
    def _create_detailed_prompt(self, sample_data: Dict, series_data: pd.Series) -> str:
        """åˆ›å»ºè¯¦ç»†çš„å¤æ ¸æç¤ºè¯"""
        
        # æå–å…³é”®ä¿¡æ¯
        title = self._safe_str(series_data.get('Title', ''))
        summary = self._safe_str(series_data.get('Summary', ''))
        overall_design = self._safe_str(series_data.get('Overall_Design', ''))
        platform = self._safe_str(series_data.get('Platform', ''))
        
        sample_id = self._safe_str(sample_data.get('Sample_id', ''))
        sample_title = self._safe_str(sample_data.get('Title', ''))
        organism = self._safe_str(sample_data.get('Organism', ''))
        source = self._safe_str(sample_data.get('Source_name', ''))
        library_strategy = self._safe_str(sample_data.get('Library_strategy', ''))
        library_source = self._safe_str(sample_data.get('Library_source', ''))
        instrument = self._safe_str(sample_data.get('Instrument_model', ''))
        
        # æå–characteristics
        characteristics = self._safe_str(sample_data.get('Characteristics', '{}'))
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©ä¿¡æ¯å­¦æ•°æ®è´¨é‡å®¡æ ¸ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹GEOæ•°æ®é›†çš„æ ·æœ¬ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦åŒæ—¶æ»¡è¶³ä»¥ä¸‹ä¸‰ä¸ªä¸¥æ ¼æ ‡å‡†ï¼š

æ ‡å‡†1: å¿…é¡»æ˜¯äººç±»(Homo sapiens)æ•°æ®
æ ‡å‡†2: å¿…é¡»æ˜¯å•ç»†èƒåˆ†è¾¨ç‡(single-cell resolution)çš„æ•°æ®
æ ‡å‡†3: å¿…é¡»æ˜¯RNAæµ‹åº(RNA-seq)æ•°æ®

ã€ç ”ç©¶é¡¹ç›®ä¿¡æ¯ã€‘
æ ‡é¢˜: {title}
æ‘˜è¦: {summary[:800]}
æ€»ä½“è®¾è®¡: {overall_design[:400]}
æµ‹åºå¹³å°: {platform}

ã€æ ·æœ¬è¯¦ç»†ä¿¡æ¯ã€‘
æ ·æœ¬ID: {sample_id}
æ ·æœ¬æ ‡é¢˜: {sample_title}
ç‰©ç§: {organism}
æ ·æœ¬æ¥æº: {source}
æµ‹åºç­–ç•¥: {library_strategy}
æ–‡åº“æ¥æº: {library_source}
æµ‹åºä»ªå™¨: {instrument}
æ ·æœ¬ç‰¹å¾: {characteristics[:400]}

ã€é‡è¦åˆ¤æ–­ä¾æ®ã€‘
1. äººç±»æ•°æ®åˆ¤æ–­:
   - ç‰©ç§å¿…é¡»æ˜ç¡®ä¸º "Homo sapiens" æˆ– "human"
   - å¦‚æœæ˜¯å…¶ä»–ç‰©ç§(å°é¼ ã€å¤§é¼ ç­‰)åˆ™ä¸ç¬¦åˆ
   
2. å•ç»†èƒåˆ¤æ–­:
   - å¿…é¡»åŒ…å« "single cell", "single-cell", "scRNA-seq" ç­‰å…³é”®è¯
   - å¦‚æœæ˜¯ "bulk RNA-seq" åˆ™ä¸ç¬¦åˆ
   - å¦‚æœåªæ˜¯ "spatial transcriptomics" éœ€è¦è°¨æ…åˆ¤æ–­
   
3. RNA-seqåˆ¤æ–­:
   - å¿…é¡»æ˜¯è½¬å½•ç»„æµ‹åºï¼Œæµ‹åºç­–ç•¥åº”åŒ…å« "RNA-Seq" æˆ–ç›¸å…³æœ¯è¯­
   - å¦‚æœæ˜¯ ATAC-seq, ChIP-seq, WGS, WES ç­‰åˆ™ä¸ç¬¦åˆ
   - å¦‚æœæ˜¯ multiome æ•°æ®ï¼Œéœ€è¦ç¡®è®¤åŒ…å«RNAéƒ¨åˆ†

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·ä»¥ä¸¥æ ¼çš„JSONæ ¼å¼è¿”å›ç»“æœï¼ˆåªè¿”å›JSONï¼Œä¸è¦ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š

{{
    "is_human": true/false,
    "is_single_cell": true/false, 
    "is_rna_seq": true/false,
    "confidence": 0.0-1.0,
    "detailed_analysis": {{
        "organism_check": "å¯¹ç‰©ç§çš„è¯¦ç»†åˆ†æ",
        "cell_type_check": "å¯¹å•ç»†èƒ/bulkçš„è¯¦ç»†åˆ†æ",
        "seq_type_check": "å¯¹æµ‹åºç±»å‹çš„è¯¦ç»†åˆ†æ",
        "key_evidence": "æ”¯æŒåˆ¤æ–­çš„å…³é”®è¯æ®",
        "concerns": "éœ€è¦æ³¨æ„çš„ç–‘ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰"
    }},
    "final_decision": "ACCEPT/REJECT/UNCERTAIN",
    "reason": "ç»¼åˆåˆ¤æ–­çš„ç®€è¦è¯´æ˜"
}}

æ³¨æ„ï¼š
- confidenceæ˜¯ä½ å¯¹åˆ¤æ–­çš„æ•´ä½“ç½®ä¿¡åº¦(0.0-1.0)
- final_decisionä¸­: ACCEPTè¡¨ç¤ºç¬¦åˆæ‰€æœ‰ä¸‰ä¸ªæ ‡å‡†ï¼ŒREJECTè¡¨ç¤ºæ˜ç¡®ä¸ç¬¦åˆï¼ŒUNCERTAINè¡¨ç¤ºæ— æ³•ç¡®å®š
- å¦‚æœä¸‰ä¸ªæ ‡å‡†æœ‰ä»»ä½•ä¸€ä¸ªä¸æ»¡è¶³ï¼Œfinal_decisionåº”ä¸ºREJECT
"""
        
        return prompt
    
    def reverify_sample(self, sample_data: Dict, max_retries: int = 3) -> Dict:
        """
        å¤æ ¸å•ä¸ªæ ·æœ¬
        
        Returns:
        --------
        dict: å¤æ ¸ç»“æœ
        """
        series_id = self._safe_str(sample_data.get('Series_id', ''))
        
        # è·å–Seriesä¿¡æ¯
        series_info = self.series_df[self.series_df['Series_id'] == series_id]
        if len(series_info) == 0:
            return {
                'sample_id': self._safe_str(sample_data.get('Sample_id', '')),
                'series_id': series_id,
                'reverify_result': 'ERROR',
                'confidence': 0.0,
                'reason': 'Seriesä¿¡æ¯ç¼ºå¤±',
                'detailed_analysis': {}
            }
        
        series_info = series_info.iloc[0]
        
        # åˆ›å»ºè¯¦ç»†æç¤ºè¯
        prompt = self._create_detailed_prompt(sample_data, series_info)
        
        for attempt in range(max_retries):
            try:
                # è°ƒç”¨API
                self.kimi_client._rate_limit()
                
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.kimi_client._get_next_key()}"
                }
                
                data = {
                    "model": "moonshot-v1-32k",  # ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ä»¥å¤„ç†è¯¦ç»†ä¿¡æ¯
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.2  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„ç»“æœ
                }
                
                import requests
                response = requests.post(
                    self.kimi_client.base_url, 
                    headers=headers, 
                    json=data, 
                    timeout=45
                )
                response.raise_for_status()
                result = response.json()
                
                content = result['choices'][0]['message']['content']
                
                # æå–JSON
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    parsed_result = json.loads(json_match.group())
                    
                    # æ·»åŠ æ ·æœ¬æ ‡è¯†
                    parsed_result['sample_id'] = self._safe_str(sample_data.get('Sample_id', ''))
                    parsed_result['series_id'] = series_id
                    
                    return parsed_result
                else:
                    if attempt == max_retries - 1:
                        return self._get_error_result(sample_data, 'APIè¿”å›æ ¼å¼é”™è¯¯')
                    time.sleep(1)
                    
            except requests.exceptions.Timeout:
                if attempt == max_retries - 1:
                    return self._get_error_result(sample_data, 'APIè¯·æ±‚è¶…æ—¶')
                time.sleep(2)
                
            except Exception as e:
                if attempt == max_retries - 1:
                    return self._get_error_result(sample_data, f'é”™è¯¯: {str(e)}')
                time.sleep(1)
        
        return self._get_error_result(sample_data, 'è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°')
    
    def _get_error_result(self, sample_data: Dict, reason: str) -> Dict:
        """è¿”å›é”™è¯¯ç»“æœ"""
        return {
            'sample_id': self._safe_str(sample_data.get('Sample_id', '')),
            'series_id': self._safe_str(sample_data.get('Series_id', '')),
            'reverify_result': 'ERROR',
            'confidence': 0.0,
            'reason': reason,
            'detailed_analysis': {}
        }
    
    def batch_reverify(self, samples_data: List[Dict], batch_size: int = 15) -> List[Dict]:
        """
        æ‰¹é‡å¤æ ¸æ ·æœ¬
        
        Parameters:
        -----------
        samples_data : List[Dict]
            å¾…å¤æ ¸çš„æ ·æœ¬æ•°æ®åˆ—è¡¨
        batch_size : int
            æ‰¹å¤„ç†å¤§å°
        
        Returns:
        --------
        List[Dict]: å¤æ ¸ç»“æœåˆ—è¡¨
        """
        print(f"\nå¼€å§‹æ‰¹é‡å¤æ ¸ {len(samples_data)} ä¸ªæ ·æœ¬...")
        
        results = []
        total = len(samples_data)
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total, batch_size):
            batch = samples_data[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total + batch_size - 1) // batch_size
            
            print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªæ ·æœ¬)")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
            with ThreadPoolExecutor(max_workers=self.kimi_client.max_workers) as executor:
                futures = {
                    executor.submit(self.reverify_sample, sample): idx
                    for idx, sample in enumerate(batch)
                }
                
                batch_results = [None] * len(batch)
                completed = 0
                
                for future in as_completed(futures):
                    idx = futures[future]
                    try:
                        batch_results[idx] = future.result()
                        completed += 1
                        print(f"\r  è¿›åº¦: {completed}/{len(batch)}", end='')
                    except Exception as e:
                        batch_results[idx] = self._get_error_result(
                            batch[idx], 
                            f'å¹¶å‘å¤„ç†é”™è¯¯: {str(e)}'
                        )
                
                results.extend(batch_results)
            
            # æ‰¹æ¬¡é—´ç­‰å¾…
            if i + batch_size < total:
                print(f"\n  ç­‰å¾… 2 ç§’åç»§ç»­...")
                time.sleep(2)
        
        print(f"\n\næ‰¹é‡å¤æ ¸å®Œæˆ!")
        return results
    
    def classify_reverify_results(self, reverify_results: List[Dict], 
                                  original_samples: List[Dict]) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        æ ¹æ®å¤æ ¸ç»“æœåˆ†ç±»æ ·æœ¬
        
        Parameters:
        -----------
        reverify_results : List[Dict]
            å¤æ ¸ç»“æœ
        original_samples : List[Dict]
            åŸå§‹æ ·æœ¬æ•°æ®
        
        Returns:
        --------
        tuple: (confirmed_clean, need_manual_review, confirmed_removed)
        """
        print("\nåˆ†ç±»å¤æ ¸ç»“æœ...")
        
        # åˆ›å»ºsample_idåˆ°åŸå§‹æ•°æ®çš„æ˜ å°„
        sample_map = {
            self._safe_str(s.get('Sample_id', '')): s 
            for s in original_samples
        }
        
        for result in reverify_results:
            sample_id = result.get('sample_id', '')
            original_data = sample_map.get(sample_id, {})
            
            # åˆå¹¶åŸå§‹æ•°æ®å’Œå¤æ ¸ç»“æœ
            merged_data = {**original_data, **result}
            
            final_decision = result.get('final_decision', 'UNCERTAIN')
            confidence = result.get('confidence', 0.0)
            
            # åˆ†ç±»é€»è¾‘
            if final_decision == 'ACCEPT':
                if confidence >= self.confidence_threshold_high:
                    # é«˜ç½®ä¿¡åº¦æ¥å—
                    merged_data['reverify_status'] = 'Confirmed Clean (High Confidence)'
                    self.confirmed_clean.append(merged_data)
                elif confidence >= self.confidence_threshold_low:
                    # ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œéœ€è¦äººå·¥å¤å®¡
                    merged_data['reverify_status'] = 'Need Manual Review (Medium Confidence Accept)'
                    self.need_manual_review.append(merged_data)
                else:
                    # ä½ç½®ä¿¡åº¦ï¼Œéœ€è¦äººå·¥å¤å®¡
                    merged_data['reverify_status'] = 'Need Manual Review (Low Confidence Accept)'
                    self.need_manual_review.append(merged_data)
            
            elif final_decision == 'REJECT':
                if confidence >= self.confidence_threshold_high:
                    # é«˜ç½®ä¿¡åº¦æ‹’ç»
                    merged_data['reverify_status'] = 'Confirmed Removed (High Confidence)'
                    self.confirmed_removed.append(merged_data)
                elif confidence >= self.confidence_threshold_low:
                    # ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œéœ€è¦äººå·¥å¤å®¡
                    merged_data['reverify_status'] = 'Need Manual Review (Medium Confidence Reject)'
                    self.need_manual_review.append(merged_data)
                else:
                    # ä½ç½®ä¿¡åº¦ï¼Œéœ€è¦äººå·¥å¤å®¡
                    merged_data['reverify_status'] = 'Need Manual Review (Low Confidence Reject)'
                    self.need_manual_review.append(merged_data)
            
            else:  # UNCERTAIN or ERROR
                merged_data['reverify_status'] = 'Need Manual Review (Uncertain)'
                self.need_manual_review.append(merged_data)
        
        print(f"\nåˆ†ç±»å®Œæˆ:")
        print(f"  âœ“ ç¡®è®¤ä¸ºæ¸…æ´æ•°æ®: {len(self.confirmed_clean)} ä¸ªæ ·æœ¬")
        print(f"  âš  éœ€è¦äººå·¥å¤å®¡: {len(self.need_manual_review)} ä¸ªæ ·æœ¬")
        print(f"  âœ— ç¡®è®¤åº”ç§»é™¤: {len(self.confirmed_removed)} ä¸ªæ ·æœ¬")
        
        return self.confirmed_clean, self.need_manual_review, self.confirmed_removed
    
    def save_reverify_results(self, output_dir: str = "reverification_report"):
        """ä¿å­˜å¤æ ¸ç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        print(f"\nä¿å­˜å¤æ ¸ç»“æœåˆ°: {output_dir}")
        
        # 1. ç¡®è®¤ä¸ºæ¸…æ´æ•°æ®
        if self.confirmed_clean:
            df_clean = pd.DataFrame(self.confirmed_clean)
            clean_file = os.path.join(output_dir, f'reverify_confirmed_clean_{timestamp}.csv')
            df_clean.to_csv(clean_file, index=False, encoding='utf-8-sig')
            print(f"  âœ“ ç¡®è®¤æ¸…æ´: {clean_file}")
        
        # 2. éœ€è¦äººå·¥å¤å®¡
        if self.need_manual_review:
            df_review = pd.DataFrame(self.need_manual_review)
            review_file = os.path.join(output_dir, f'reverify_need_manual_review_{timestamp}.csv')
            df_review.to_csv(review_file, index=False, encoding='utf-8-sig')
            print(f"  âš  éœ€è¦å¤å®¡: {review_file}")
            
            # åˆ›å»ºç®€åŒ–ç‰ˆä¾›äººå·¥å®¡æ ¸
            review_cols = [
                'Sample_id', 'Series_id', 'Title', 'Organism', 
                'Library_strategy', 'reverify_result', 'confidence',
                'reason', 'reverify_status'
            ]
            available_cols = [col for col in review_cols if col in df_review.columns]
            df_review_simple = df_review[available_cols]
            review_simple_file = os.path.join(output_dir, f'reverify_review_simplified_{timestamp}.csv')
            df_review_simple.to_csv(review_simple_file, index=False, encoding='utf-8-sig')
            print(f"  âš  ç®€åŒ–ç‰ˆå¤å®¡è¡¨: {review_simple_file}")
        
        # 3. ç¡®è®¤åº”ç§»é™¤
        if self.confirmed_removed:
            df_removed = pd.DataFrame(self.confirmed_removed)
            removed_file = os.path.join(output_dir, f'reverify_confirmed_removed_{timestamp}.csv')
            df_removed.to_csv(removed_file, index=False, encoding='utf-8-sig')
            print(f"  âœ— ç¡®è®¤ç§»é™¤: {removed_file}")
        
        # 4. ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š
        analysis_report = {
            'total_reverified': len(self.confirmed_clean) + len(self.need_manual_review) + len(self.confirmed_removed),
            'confirmed_clean_count': len(self.confirmed_clean),
            'need_review_count': len(self.need_manual_review),
            'confirmed_removed_count': len(self.confirmed_removed),
            'confidence_threshold_high': self.confidence_threshold_high,
            'confidence_threshold_low': self.confidence_threshold_low,
            'timestamp': timestamp
        }
        
        report_file = os.path.join(output_dir, f'reverify_summary_{timestamp}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_report, f, indent=2, ensure_ascii=False)
        
        print(f"  ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {report_file}")
        
        return {
            'clean_file': clean_file if self.confirmed_clean else None,
            'review_file': review_file if self.need_manual_review else None,
            'removed_file': removed_file if self.confirmed_removed else None,
            'report_file': report_file
        }

class SmartReVerifier:
    """æ™ºèƒ½å¤æ ¸å™¨ - ä½¿ç”¨è§„åˆ™+é‡‡æ ·+APIçš„æ··åˆç­–ç•¥"""
    
    def __init__(self, kimi_client, series_df: pd.DataFrame):
        self.kimi_client = kimi_client
        self.series_df = series_df
        
        # ç»“æœå­˜å‚¨
        self.confirmed_clean = []       # è§„åˆ™ç›´æ¥ç¡®è®¤æ¸…æ´
        self.confirmed_removed = []     # è§„åˆ™ç›´æ¥ç¡®è®¤ç§»é™¤
        self.need_api_check = []        # éœ€è¦APIæ£€æŸ¥
        self.need_manual_review = []    # æœ€ç»ˆéœ€è¦äººå·¥å¤å®¡
        
        # å¼ºè§„åˆ™æ¨¡å¼
        self.strict_human_keywords = ['homo sapiens', 'human']
        self.strict_exclude_organisms = ['mus musculus', 'mouse', 'rattus', 'rat', 
                                         'drosophila', 'zebrafish', 'danio rerio']
        
        self.strict_sc_keywords = ['10x genomics', 'chromium', 'drop-seq', 
                                   'smart-seq2', 'cel-seq', 'scrna-seq', 'scrnaseq']
        self.strict_bulk_keywords = ['bulk rna-seq', 'bulk transcriptome', 'bulk rna']
        
        self.strict_rna_keywords = ['rna-seq', 'rna seq', 'transcriptome sequencing']
        self.strict_non_rna = ['atac-seq', 'chip-seq', 'dnase-seq', 'wgs', 'wes', 
                               'whole genome sequencing', 'whole exome']
    
    def _safe_str(self, value) -> str:
        if pd.isna(value) or value is None:
            return ''
        return str(value)
    
    def _safe_lower(self, value) -> str:
        return self._safe_str(value).lower()
    
    def calculate_priority_score(self, sample_data: Dict) -> Tuple[float, Dict]:
        """
        è®¡ç®—æ ·æœ¬çš„ä¼˜å…ˆçº§åˆ†æ•°ï¼Œå†³å®šæ˜¯å¦éœ€è¦APIæ£€æŸ¥
        
        Returns:
        --------
        tuple: (priority_score, confidence_indicators)
            priority_score: 0-100ï¼Œåˆ†æ•°è¶Šé«˜è¶Šéœ€è¦APIæ£€æŸ¥
            confidence_indicators: å„é¡¹æŒ‡æ ‡çš„è¯¦ç»†ä¿¡æ¯
        """
        score = 0
        indicators = {
            'organism_clarity': 0,     # ç‰©ç§æ˜ç¡®åº¦ (0=ä¸æ˜ç¡®, 1=æ˜ç¡®äººç±», -1=æ˜ç¡®éäººç±»)
            'sc_clarity': 0,           # å•ç»†èƒæ˜ç¡®åº¦
            'rna_clarity': 0,          # RNA-seqæ˜ç¡®åº¦
            'text_quality': 0,         # æ–‡æœ¬ä¿¡æ¯è´¨é‡
            'conflict_signals': 0      # å†²çªä¿¡å·æ•°é‡
        }
        
        # è·å–æ–‡æœ¬ä¿¡æ¯
        title = self._safe_lower(sample_data.get('Title', ''))
        organism = self._safe_lower(sample_data.get('Organism', ''))
        library_strategy = self._safe_lower(sample_data.get('Library_strategy', ''))
        source = self._safe_lower(sample_data.get('Source_name', ''))
        
        series_id = self._safe_str(sample_data.get('Series_id', ''))
        series_info = self.series_df[self.series_df['Series_id'] == series_id]
        if len(series_info) > 0:
            series_title = self._safe_lower(series_info.iloc[0].get('Title', ''))
            summary = self._safe_lower(series_info.iloc[0].get('Summary', ''))
        else:
            series_title = ''
            summary = ''
        
        full_text = f"{title} {series_title} {summary[:300]} {organism} {source}"
        
        # 1. ç‰©ç§æ˜ç¡®åº¦æ£€æŸ¥
        if any(kw in organism for kw in self.strict_human_keywords):
            indicators['organism_clarity'] = 1  # æ˜ç¡®äººç±»
        elif any(kw in organism for kw in self.strict_exclude_organisms):
            indicators['organism_clarity'] = -1  # æ˜ç¡®éäººç±»
            score += 0  # ä¸éœ€è¦APIæ£€æŸ¥ï¼Œç›´æ¥æ’é™¤
        else:
            indicators['organism_clarity'] = 0  # ä¸æ˜ç¡®
            score += 40  # éœ€è¦APIæ£€æŸ¥
        
        # 2. å•ç»†èƒæ˜ç¡®åº¦
        sc_count = sum(1 for kw in self.strict_sc_keywords if kw in full_text)
        bulk_count = sum(1 for kw in self.strict_bulk_keywords if kw in full_text)
        
        if sc_count >= 2:
            indicators['sc_clarity'] = 1  # æ˜ç¡®å•ç»†èƒ
        elif bulk_count >= 1:
            indicators['sc_clarity'] = -1  # æ˜ç¡®bulk
            score += 0  # ä¸éœ€è¦API
        elif sc_count == 1:
            indicators['sc_clarity'] = 0.5  # å¯èƒ½å•ç»†èƒ
            score += 30
        else:
            indicators['sc_clarity'] = 0  # ä¸æ˜ç¡®
            score += 50  # é«˜ä¼˜å…ˆçº§APIæ£€æŸ¥
        
        # 3. RNA-seqæ˜ç¡®åº¦
        rna_count = sum(1 for kw in self.strict_rna_keywords if kw in library_strategy or kw in full_text)
        non_rna_count = sum(1 for kw in self.strict_non_rna if kw in full_text)
        
        if rna_count >= 1 and non_rna_count == 0:
            indicators['rna_clarity'] = 1  # æ˜ç¡®RNA-seq
        elif non_rna_count >= 1:
            indicators['rna_clarity'] = -1  # æ˜ç¡®éRNA-seq
            score += 0  # ä¸éœ€è¦API
        else:
            indicators['rna_clarity'] = 0  # ä¸æ˜ç¡®
            score += 35
        
        # 4. æ–‡æœ¬è´¨é‡
        text_length = len(summary) + len(title)
        if text_length > 200:
            indicators['text_quality'] = 1
        elif text_length > 50:
            indicators['text_quality'] = 0.5
            score += 10
        else:
            indicators['text_quality'] = 0
            score += 20  # æ–‡æœ¬å°‘ï¼Œéš¾åˆ¤æ–­ï¼Œéœ€è¦API
        
        # 5. å†²çªä¿¡å·æ£€æŸ¥
        conflicts = 0
        if 'single cell' in full_text and 'bulk' in full_text:
            conflicts += 1
        if 'spatial' in full_text and 'single cell' in full_text:
            conflicts += 1
        if 'multiome' in full_text or 'cite-seq' in full_text:
            conflicts += 1
        
        indicators['conflict_signals'] = conflicts
        score += conflicts * 25  # æœ‰å†²çªä¿¡å·ï¼Œæé«˜ä¼˜å…ˆçº§
        
        return min(score, 100), indicators
    
    def rule_based_classification(self, sample_data: Dict) -> str:
        """
        åŸºäºå¼ºè§„åˆ™çš„åˆ†ç±»
        
        Returns:
        --------
        str: 'ACCEPT', 'REJECT', 'UNCERTAIN'
        """
        organism = self._safe_lower(sample_data.get('Organism', ''))
        library_strategy = self._safe_lower(sample_data.get('Library_strategy', ''))
        
        series_id = self._safe_str(sample_data.get('Series_id', ''))
        series_info = self.series_df[self.series_df['Series_id'] == series_id]
        if len(series_info) > 0:
            title = self._safe_lower(series_info.iloc[0].get('Title', ''))
            summary = self._safe_lower(series_info.iloc[0].get('Summary', ''))
        else:
            title = ''
            summary = ''
        
        full_text = f"{title} {summary} {organism} {library_strategy}"
        
        # å¼ºæ’é™¤è§„åˆ™
        # 1. éäººç±»ç‰©ç§
        if any(kw in organism for kw in self.strict_exclude_organisms):
            return 'REJECT'
        
        # 2. æ˜ç¡®çš„bulkæ•°æ®
        if any(kw in full_text for kw in self.strict_bulk_keywords):
            return 'REJECT'
        
        # 3. æ˜ç¡®çš„éRNAæµ‹åº
        if any(kw in full_text for kw in self.strict_non_rna):
            return 'REJECT'
        
        # å¼ºæ¥å—è§„åˆ™ï¼ˆéœ€è¦åŒæ—¶æ»¡è¶³ï¼‰
        is_human = any(kw in organism for kw in self.strict_human_keywords)
        is_sc = any(kw in full_text for kw in self.strict_sc_keywords)
        is_rna = any(kw in full_text for kw in self.strict_rna_keywords)
        
        if is_human and is_sc and is_rna:
            # æ£€æŸ¥æ˜¯å¦æœ‰å†²çª
            has_conflict = ('spatial' in full_text or 'multiome' in full_text or 
                          'cite-seq' in full_text)
            if not has_conflict:
                return 'ACCEPT'
        
        return 'UNCERTAIN'
    
    def smart_sampling_strategy(self, uncertain_samples: List[Dict], 
                                max_api_calls: int = 1000) -> Tuple[List[Dict], List[Dict]]:
        """
        æ™ºèƒ½é‡‡æ ·ç­–ç•¥ - é€‰æ‹©æœ€éœ€è¦APIæ£€æŸ¥çš„æ ·æœ¬
        
        Parameters:
        -----------
        uncertain_samples : List[Dict]
            ä¸ç¡®å®šçš„æ ·æœ¬åˆ—è¡¨
        max_api_calls : int
            æœ€å¤§APIè°ƒç”¨æ¬¡æ•°
        
        Returns:
        --------
        tuple: (samples_for_api, samples_for_manual)
        """
        print(f"\nå¼€å§‹æ™ºèƒ½é‡‡æ · ({len(uncertain_samples)} ä¸ªä¸ç¡®å®šæ ·æœ¬)...")
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¼˜å…ˆçº§
        samples_with_priority = []
        for sample in uncertain_samples:
            priority_score, indicators = self.calculate_priority_score(sample)
            samples_with_priority.append({
                'sample': sample,
                'priority_score': priority_score,
                'indicators': indicators
            })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        samples_with_priority.sort(key=lambda x: x['priority_score'], reverse=True)
        
        # é€‰æ‹©é«˜ä¼˜å…ˆçº§æ ·æœ¬è¿›è¡ŒAPIæ£€æŸ¥
        samples_for_api = []
        samples_for_manual = []
        
        for item in samples_with_priority:
            if len(samples_for_api) < max_api_calls and item['priority_score'] > 30:
                samples_for_api.append(item['sample'])
            else:
                # ä½ä¼˜å…ˆçº§çš„ç›´æ¥æ ‡è®°ä¸ºéœ€è¦äººå·¥å¤å®¡
                samples_for_manual.append(item['sample'])
        
        print(f"  é€‰æ‹© {len(samples_for_api)} ä¸ªé«˜ä¼˜å…ˆçº§æ ·æœ¬è¿›è¡ŒAPIæ£€æŸ¥")
        print(f"  {len(samples_for_manual)} ä¸ªä½ä¼˜å…ˆçº§æ ·æœ¬ç›´æ¥æ ‡è®°ä¸ºäººå·¥å¤å®¡")
        
        # æ˜¾ç¤ºä¼˜å…ˆçº§åˆ†å¸ƒ
        if samples_with_priority:
            scores = [x['priority_score'] for x in samples_with_priority]
            print(f"\n  ä¼˜å…ˆçº§åˆ†æ•°åˆ†å¸ƒ:")
            print(f"    æœ€é«˜åˆ†: {max(scores):.1f}")
            print(f"    æœ€ä½åˆ†: {min(scores):.1f}")
            print(f"    å¹³å‡åˆ†: {np.mean(scores):.1f}")
            print(f"    ä¸­ä½æ•°: {np.median(scores):.1f}")
        
        return samples_for_api, samples_for_manual
    
    def batch_reverify_with_api(self, samples: List[Dict], batch_size: int = 15) -> List[Dict]:
        
        reverifier = DataReVerifier(
            kimi_client=self.kimi_client,
            series_df=self.series_df
        )
        
        return reverifier.batch_reverify(samples, batch_size)
    
    def classify_all_samples(self, flagged_samples: List[Dict], 
                            removed_samples: List[Dict],
                            max_api_calls: int = 1000) -> Dict:
        """
        æ™ºèƒ½åˆ†ç±»æ‰€æœ‰æ ·æœ¬
        
        Parameters:
        -----------
        flagged_samples : List[Dict]
            å¾…å¤æ ¸æ ·æœ¬
        removed_samples : List[Dict]
            å·²ç§»é™¤æ ·æœ¬
        max_api_calls : int
            æœ€å¤§APIè°ƒç”¨æ¬¡æ•°é¢„ç®—
        
        Returns:
        --------
        dict: åˆ†ç±»ç»“æœç»Ÿè®¡
        """
        print("\n" + "=" * 80)
        print("æ™ºèƒ½å¤æ ¸æµç¨‹ - ä¸‰é˜¶æ®µå¤„ç†")
        print("=" * 80)
        
        all_samples = flagged_samples + removed_samples
        total_samples = len(all_samples)
        
        print(f"\næ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  - å¾…å¤æ ¸æ ·æœ¬: {len(flagged_samples)}")
        print(f"  - å·²ç§»é™¤æ ·æœ¬: {len(removed_samples)}")
        print(f"APIè°ƒç”¨é¢„ç®—: {max_api_calls}")
        
        # ===== é˜¶æ®µ1: è§„åˆ™åˆ†ç±» =====
        print("\n" + "-" * 80)
        print("é˜¶æ®µ1: åŸºäºå¼ºè§„åˆ™çš„å¿«é€Ÿåˆ†ç±»")
        print("-" * 80)
        
        rule_accept = []
        rule_reject = []
        rule_uncertain = []
        
        for idx, sample in enumerate(all_samples, 1):
            if idx % 1000 == 0:
                print(f"\r  è¿›åº¦: {idx}/{total_samples}", end='')
            
            decision = self.rule_based_classification(sample)
            
            if decision == 'ACCEPT':
                sample['classification_method'] = 'Rule-based ACCEPT'
                sample['confidence'] = 0.9
                rule_accept.append(sample)
            elif decision == 'REJECT':
                sample['classification_method'] = 'Rule-based REJECT'
                sample['confidence'] = 0.9
                rule_reject.append(sample)
            else:
                rule_uncertain.append(sample)
        
        print(f"\n\né˜¶æ®µ1å®Œæˆ:")
        print(f"  âœ“ è§„åˆ™ç¡®è®¤æ¸…æ´: {len(rule_accept)}")
        print(f"  âœ— è§„åˆ™ç¡®è®¤ç§»é™¤: {len(rule_reject)}")
        print(f"  ? éœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­: {len(rule_uncertain)}")
        
        # ===== é˜¶æ®µ2: æ™ºèƒ½é‡‡æ · + APIæ£€æŸ¥ =====
        print("\n" + "-" * 80)
        print("é˜¶æ®µ2: æ™ºèƒ½é‡‡æ · + APIæ·±åº¦æ£€æŸ¥")
        print("-" * 80)
        
        api_samples, direct_manual = self.smart_sampling_strategy(
            rule_uncertain, 
            max_api_calls
        )
        
        api_results = []
        if api_samples:
            print(f"\næ‰§è¡ŒAPIæ£€æŸ¥ ({len(api_samples)} ä¸ªæ ·æœ¬)...")
            api_results = self.batch_reverify_with_api(api_samples, batch_size=15)
        
        # åˆ†ç±»APIç»“æœ
        api_accept = []
        api_reject = []
        api_uncertain = []
        
        for result in api_results:
            confidence = result.get('confidence', 0.0)
            decision = result.get('final_decision', 'UNCERTAIN')
            
            if decision == 'ACCEPT' and confidence >= 0.85:
                result['classification_method'] = 'API ACCEPT (High Confidence)'
                api_accept.append(result)
            elif decision == 'REJECT' and confidence >= 0.85:
                result['classification_method'] = 'API REJECT (High Confidence)'
                api_reject.append(result)
            else:
                result['classification_method'] = 'API UNCERTAIN'
                api_uncertain.append(result)
        
        print(f"\né˜¶æ®µ2å®Œæˆ:")
        print(f"  âœ“ APIç¡®è®¤æ¸…æ´: {len(api_accept)}")
        print(f"  âœ— APIç¡®è®¤ç§»é™¤: {len(api_reject)}")
        print(f"  ? APIä¸ç¡®å®š: {len(api_uncertain)}")
        
        # ===== é˜¶æ®µ3: æ±‡æ€»ç»“æœ =====
        print("\n" + "-" * 80)
        print("é˜¶æ®µ3: ç»“æœæ±‡æ€»")
        print("-" * 80)
        
        self.confirmed_clean = rule_accept + api_accept
        self.confirmed_removed = rule_reject + api_reject
        self.need_manual_review = api_uncertain + direct_manual
        
        # æ ‡è®°äººå·¥å¤å®¡çš„åŸå› 
        for sample in direct_manual:
            sample['manual_review_reason'] = 'Low priority - skipped API check'
            sample['classification_method'] = 'Manual review needed (low priority)'
        
        print(f"\næœ€ç»ˆåˆ†ç±»ç»“æœ:")
        print(f"  âœ“ ç¡®è®¤æ¸…æ´æ•°æ®: {len(self.confirmed_clean)} ({len(self.confirmed_clean)/total_samples*100:.1f}%)")
        print(f"  âœ— ç¡®è®¤ç§»é™¤æ•°æ®: {len(self.confirmed_removed)} ({len(self.confirmed_removed)/total_samples*100:.1f}%)")
        print(f"  âš  éœ€äººå·¥å¤å®¡: {len(self.need_manual_review)} ({len(self.need_manual_review)/total_samples*100:.1f}%)")
        
        print(f"\nAPIè°ƒç”¨ç»Ÿè®¡:")
        print(f"  é¢„ç®—: {max_api_calls}")
        print(f"  å®é™…ä½¿ç”¨: {len(api_samples)}")
        print(f"  èŠ‚çœ: {max_api_calls - len(api_samples)} ({(1 - len(api_samples)/max_api_calls)*100:.1f}%)")
        
        return {
            'total': total_samples,
            'confirmed_clean': len(self.confirmed_clean),
            'confirmed_removed': len(self.confirmed_removed),
            'need_manual_review': len(self.need_manual_review),
            'api_calls_used': len(api_samples),
            'api_calls_budget': max_api_calls
        }
    
    def save_results(self, output_dir: str = "smart_reverification_report"):
        """ä¿å­˜ç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        print(f"\nä¿å­˜ç»“æœåˆ°: {output_dir}")
        
        files = {}
        
        # 1. ç¡®è®¤æ¸…æ´æ•°æ®
        if self.confirmed_clean:
            df = pd.DataFrame(self.confirmed_clean)
            file_path = os.path.join(output_dir, f'confirmed_clean_{timestamp}.csv')
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            files['clean'] = file_path
            print(f"  âœ“ ç¡®è®¤æ¸…æ´: {file_path}")
        
        # 2. ç¡®è®¤ç§»é™¤æ•°æ®
        if self.confirmed_removed:
            df = pd.DataFrame(self.confirmed_removed)
            file_path = os.path.join(output_dir, f'confirmed_removed_{timestamp}.csv')
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            files['removed'] = file_path
            print(f"  âœ— ç¡®è®¤ç§»é™¤: {file_path}")
        
        # 3. éœ€äººå·¥å¤å®¡ - å®Œæ•´ç‰ˆ
        if self.need_manual_review:
            df = pd.DataFrame(self.need_manual_review)
            file_path = os.path.join(output_dir, f'need_manual_review_{timestamp}.csv')
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            files['review_full'] = file_path
            print(f"  âš  éœ€äººå·¥å¤å®¡(å®Œæ•´): {file_path}")
            
            # ç®€åŒ–ç‰ˆ
            review_cols = ['Sample_id', 'Series_id', 'Title', 'Organism', 
                          'Library_strategy', 'classification_method', 
                          'manual_review_reason']
            available_cols = [col for col in review_cols if col in df.columns]
            df_simple = df[available_cols]
            file_path_simple = os.path.join(output_dir, f'need_manual_review_simple_{timestamp}.csv')
            df_simple.to_csv(file_path_simple, index=False, encoding='utf-8-sig')
            files['review_simple'] = file_path_simple
            print(f"  âš  éœ€äººå·¥å¤å®¡(ç®€åŒ–): {file_path_simple}")
        
        # 4. ç»Ÿè®¡æŠ¥å‘Š
        report = {
            'timestamp': timestamp,
            'total_samples': len(self.confirmed_clean) + len(self.confirmed_removed) + len(self.need_manual_review),
            'confirmed_clean': len(self.confirmed_clean),
            'confirmed_removed': len(self.confirmed_removed),
            'need_manual_review': len(self.need_manual_review),
            'clean_rate': len(self.confirmed_clean) / (len(self.confirmed_clean) + len(self.confirmed_removed) + len(self.need_manual_review)) * 100
        }
        
        report_file = os.path.join(output_dir, f'summary_{timestamp}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        files['report'] = report_file
        print(f"  ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {report_file}")
        
        return files


def main_smart_reverification():
    """ä¸»å‡½æ•° - æ™ºèƒ½å¤æ ¸"""
    print("=" * 80)
    print("GEO Smart Re-verification Module v2.0")
    print("æ™ºèƒ½å¤æ ¸æ¨¡å— - æˆæœ¬ä¼˜åŒ–ç‰ˆ")
    print("=" * 80)
    
    # é…ç½®
    series_file = "/mnt/public7/pancancercol/meta_analysis/geo_metadata/comprehensive_metadata/complete_human_series_20251125_034124.csv"
    
    # è¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„æ–‡ä»¶è·¯å¾„
    flagged_file = "/mnt/public7/pancancercol/meta_analysis/geo_metadata/clean_metadata/cleaning_report/flagged_for_review_20251217_012206.csv"
    removed_file = "/mnt/public7/pancancercol/meta_analysis/geo_metadata/clean_metadata/cleaning_report/removed_data_20251217_012206.csv"
    
    # è¯»å–æ•°æ®
    print(f"\nåŠ è½½æ•°æ®...")
    series_df = pd.read_csv(series_file)
    
    flagged_samples = []
    removed_samples = []
    
    if os.path.exists(flagged_file):
        df = pd.read_csv(flagged_file)
        flagged_samples = df.to_dict('records')
        print(f"  å¾…å¤æ ¸æ ·æœ¬: {len(flagged_samples)}")
    
    if os.path.exists(removed_file):
        df = pd.read_csv(removed_file)
        removed_samples = df.to_dict('records')
        print(f"  å·²ç§»é™¤æ ·æœ¬: {len(removed_samples)}")
    
    # åˆå§‹åŒ–
    from geo_cleaner import KimiAPIClient
    api_keys = [
        "sk-UL5YodR7ZL4S9dytpfMWJgmPTXJjkeNSd7Ktq9bbEhElzDfX",
        "sk-WL1hTKhW3sYKuJjBSc1k8wxL0r8ZLcuM7YiYxFjGgVHqAXhU",
        "sk-RAkA28HIT5tiMEfKtXAgbZ9nZKweq5Bnw0WbSwwBdNX7nbi1"
    ]
    kimi_client = KimiAPIClient(api_keys)
    
    # åˆ›å»ºæ™ºèƒ½å¤æ ¸å™¨
    reverifier = SmartReVerifier(kimi_client, series_df)
    
    # æ‰§è¡Œæ™ºèƒ½åˆ†ç±»
    start_time = time.time()
    
    stats = reverifier.classify_all_samples(
        flagged_samples, 
        removed_samples,
        max_api_calls=1000  # å¯æ ¹æ®é¢„ç®—è°ƒæ•´
    )
    
    # ä¿å­˜ç»“æœ
    files = reverifier.save_results()
    
    elapsed = time.time() - start_time
    
    # æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("æ™ºèƒ½å¤æ ¸å®Œæˆ!")
    print("=" * 80)
    print(f"\næ€»è€—æ—¶: {elapsed:.2f} ç§’ ({elapsed/60:.2f} åˆ†é’Ÿ)")
    print(f"\næˆæœ¬æ•ˆç›Š:")
    print(f"  å¦‚æœå…¨éƒ¨ä½¿ç”¨API: {stats['total']} æ¬¡è°ƒç”¨")
    print(f"  å®é™…APIè°ƒç”¨: {stats['api_calls_used']} æ¬¡")
    print(f"  èŠ‚çœ: {stats['total'] - stats['api_calls_used']} æ¬¡ ({(1-stats['api_calls_used']/stats['total'])*100:.1f}%)")


if __name__ == "__main__":
    main_smart_reverification()