#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dbGaP å•ç»†èƒæµ‹åºæ•°æ®æ£€ç´¢ä¸æ•´åˆå·¥å…· v2.0
æ”¹è¿›ç‰ˆï¼šä½¿ç”¨å¤šç§ç­–ç•¥ä»NCBIè·å–dbGaPå•ç»†èƒæ•°æ®
"""

import requests
import xml.etree.ElementTree as ET
import pandas as pd
import time
import os
import json
from datetime import datetime
from pathlib import Path
import re
from typing import Dict, List, Optional, Tuple
import logging
from urllib.parse import quote
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dbgap_scraper_v2.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class dbGaPScraperV2:
    """dbGaP æ•°æ®åº“çˆ¬å–å™¨ - æ”¹è¿›ç‰ˆ"""
    
    def __init__(self, output_dir="dbgap_metadata_v2", email="your_email@example.com"):
        """
        åˆå§‹åŒ–çˆ¬å–å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            email: å¿…éœ€çš„è”ç³»é‚®ç®±
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # NCBI APIé…ç½®
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        self.email = email
        self.api_key = None  # å¦‚æœ‰API keyè¯·å¡«å…¥
        
        # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        self.request_delay = 0.34 if not self.api_key else 0.1
        
        logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"è”ç³»é‚®ç®±: {self.email}")
    
    def strategy_1_search_sra_for_dbgap(self) -> List[Dict]:
        """
        ç­–ç•¥1: é€šè¿‡SRAæ•°æ®åº“æœç´¢å…³è”åˆ°dbGaPçš„å•ç»†èƒç ”ç©¶
        
        Returns:
            ç ”ç©¶ä¿¡æ¯åˆ—è¡¨
        """
        logger.info("=" * 70)
        logger.info("ç­–ç•¥1: é€šè¿‡SRAæ•°æ®åº“æœç´¢dbGaPç›¸å…³çš„å•ç»†èƒç ”ç©¶")
        logger.info("=" * 70)
        
        # æ„å»ºSRAæœç´¢æŸ¥è¯¢
        search_terms = [
            '("single cell RNA"[Strategy] OR "single-cell RNA"[Strategy])',
            'AND "Homo sapiens"[Organism]',
            'AND "dbgap"[Filter]'  # é™å®šæœ‰dbGaPé“¾æ¥çš„æ•°æ®
        ]
        
        query = ' '.join(search_terms)
        logger.info(f"SRAæœç´¢æŸ¥è¯¢: {query}")
        
        # Step 1: æœç´¢SRA
        study_ids = self._search_ncbi_db('sra', query, max_results=1000)
        logger.info(f"âœ“ åœ¨SRAä¸­æ‰¾åˆ° {len(study_ids)} ä¸ªç›¸å…³ç ”ç©¶")
        
        if not study_ids:
            logger.warning("SRAä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç ”ç©¶")
            return []
        
        # Step 2: è·å–SRAè¯¦æƒ…å¹¶æå–dbGaPé“¾æ¥
        studies = []
        for idx, sra_id in enumerate(study_ids[:100], 1):  # å…ˆå¤„ç†å‰100ä¸ª
            logger.info(f"[{idx}/{min(len(study_ids), 100)}] å¤„ç†SRAè®°å½•: {sra_id}")
            
            study_info = self._fetch_sra_details(sra_id)
            if study_info and study_info.get('dbgap_accession'):
                studies.append(study_info)
                logger.info(f"  âœ“ æ‰¾åˆ°dbGaPç ”ç©¶: {study_info['dbgap_accession']}")
            
            if idx % 10 == 0:
                self._save_json(studies, "strategy1_intermediate.json")
            
            time.sleep(self.request_delay)
        
        logger.info(f"âœ“ ç­–ç•¥1å®Œæˆï¼Œæ‰¾åˆ° {len(studies)} ä¸ªdbGaPç ”ç©¶")
        self._save_json(studies, "strategy1_results.json")
        
        return studies
    
    def strategy_2_search_bioproject(self) -> List[Dict]:
        """
        ç­–ç•¥2: é€šè¿‡BioProjectæœç´¢dbGaPé¡¹ç›®
        
        Returns:
            ç ”ç©¶ä¿¡æ¯åˆ—è¡¨
        """
        logger.info("=" * 70)
        logger.info("ç­–ç•¥2: é€šè¿‡BioProjectæœç´¢dbGaPé¡¹ç›®")
        logger.info("=" * 70)
        
        # æœç´¢BioProject
        search_terms = [
            '(single cell[Title] OR scRNA[Title] OR "single-cell"[Title])',
            'AND "Homo sapiens"[Organism]',
            'AND dbgap[Properties]'
        ]
        
        query = ' '.join(search_terms)
        logger.info(f"BioProjectæœç´¢æŸ¥è¯¢: {query}")
        
        project_ids = self._search_ncbi_db('bioproject', query, max_results=500)
        logger.info(f"âœ“ æ‰¾åˆ° {len(project_ids)} ä¸ªBioProject")
        
        studies = []
        for idx, project_id in enumerate(project_ids, 1):
            logger.info(f"[{idx}/{len(project_ids)}] å¤„ç†BioProject: {project_id}")
            
            study_info = self._fetch_bioproject_details(project_id)
            if study_info:
                studies.append(study_info)
                logger.info(f"  âœ“ æˆåŠŸè§£æé¡¹ç›®")
            
            if idx % 10 == 0:
                self._save_json(studies, "strategy2_intermediate.json")
            
            time.sleep(self.request_delay)
        
        logger.info(f"âœ“ ç­–ç•¥2å®Œæˆï¼Œæ‰¾åˆ° {len(studies)} ä¸ªç ”ç©¶")
        self._save_json(studies, "strategy2_results.json")
        
        return studies
    
    def strategy_3_direct_dbgap_portal(self) -> List[Dict]:
        """
        ç­–ç•¥3: ç›´æ¥è®¿é—®dbGaPç½‘é¡µAPI
        
        Returns:
            ç ”ç©¶ä¿¡æ¯åˆ—è¡¨
        """
        logger.info("=" * 70)
        logger.info("ç­–ç•¥3: ç›´æ¥è®¿é—®dbGaPç½‘é¡µAPI")
        logger.info("=" * 70)
        
        # dbGaPæä¾›äº†ä¸€ä¸ªJSONæ ¼å¼çš„ç ”ç©¶åˆ—è¡¨
        dbgap_api_url = "https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/GetStudyListJson.cgi"
        
        try:
            logger.info(f"è¯·æ±‚dbGaP API: {dbgap_api_url}")
            response = requests.get(dbgap_api_url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            all_studies = data.get('studies', [])
            
            logger.info(f"âœ“ è·å–åˆ° {len(all_studies)} ä¸ªdbGaPç ”ç©¶")
            
            # ç­›é€‰å•ç»†èƒç›¸å…³ç ”ç©¶
            sc_studies = []
            sc_keywords = [
                'single cell', 'single-cell', 'scRNA', 'scRNA-seq',
                '10x', 'droplet', 'drop-seq', 'chromium'
            ]
            
            for study in all_studies:
                study_text = f"{study.get('study_name', '')} {study.get('disease', '')}".lower()
                
                if any(keyword.lower() in study_text for keyword in sc_keywords):
                    # æ ‡å‡†åŒ–ç ”ç©¶ä¿¡æ¯
                    study_info = self._parse_dbgap_json_study(study)
                    sc_studies.append(study_info)
                    logger.info(f"  âœ“ æ‰¾åˆ°å•ç»†èƒç ”ç©¶: {study.get('study_accession')}")
            
            logger.info(f"âœ“ ç­–ç•¥3å®Œæˆï¼Œæ‰¾åˆ° {len(sc_studies)} ä¸ªå•ç»†èƒç ”ç©¶")
            self._save_json(sc_studies, "strategy3_results.json")
            
            return sc_studies
            
        except Exception as e:
            logger.error(f"âœ— ç­–ç•¥3å¤±è´¥: {str(e)}")
            return []
    
    def strategy_4_pubmed_linked_search(self) -> List[Dict]:
        """
        ç­–ç•¥4: é€šè¿‡PubMedæ–‡çŒ®æœç´¢å…³è”çš„dbGaPæ•°æ®
        
        Returns:
            ç ”ç©¶ä¿¡æ¯åˆ—è¡¨
        """
        logger.info("=" * 70)
        logger.info("ç­–ç•¥4: é€šè¿‡PubMedæ–‡çŒ®æœç´¢dbGaPæ•°æ®")
        logger.info("=" * 70)
        
        # æœç´¢ç›¸å…³æ–‡çŒ®
        search_terms = [
            '("single cell RNA sequencing"[Title/Abstract] OR "scRNA-seq"[Title/Abstract])',
            'AND "Homo sapiens"[Organism]',
            'AND "dbgap"[Text Word]'
        ]
        
        query = ' '.join(search_terms)
        logger.info(f"PubMedæœç´¢æŸ¥è¯¢: {query}")
        
        pmids = self._search_ncbi_db('pubmed', query, max_results=200)
        logger.info(f"âœ“ æ‰¾åˆ° {len(pmids)} ç¯‡ç›¸å…³æ–‡çŒ®")
        
        studies = []
        dbgap_accessions = set()
        
        for idx, pmid in enumerate(pmids, 1):
            logger.info(f"[{idx}/{len(pmids)}] å¤„ç†æ–‡çŒ® PMID: {pmid}")
            
            # è·å–æ–‡çŒ®è¯¦æƒ…
            pub_info = self._fetch_pubmed_details(pmid)
            
            # æå–dbGaP accession
            accessions = self._extract_dbgap_accessions(pub_info)
            
            for accession in accessions:
                if accession not in dbgap_accessions:
                    dbgap_accessions.add(accession)
                    
                    study_info = {
                        'dbgap_accession': accession,
                        'source_pmid': pmid,
                        'source_strategy': 'pubmed_linked',
                        'publication': pub_info
                    }
                    
                    studies.append(study_info)
                    logger.info(f"  âœ“ æå–åˆ°dbGaP accession: {accession}")
            
            if idx % 10 == 0:
                self._save_json(studies, "strategy4_intermediate.json")
            
            time.sleep(self.request_delay)
        
        logger.info(f"âœ“ ç­–ç•¥4å®Œæˆï¼Œæ‰¾åˆ° {len(studies)} ä¸ªdbGaPç ”ç©¶")
        self._save_json(studies, "strategy4_results.json")
        
        return studies
    
    def _search_ncbi_db(self, db: str, query: str, max_results: int = 500) -> List[str]:
        """é€šç”¨NCBIæ•°æ®åº“æœç´¢"""
        params = {
            'db': db,
            'term': query,
            'retmax': max_results,
            'retmode': 'json',
            'email': self.email
        }
        
        if self.api_key:
            params['api_key'] = self.api_key
        
        try:
            response = requests.get(f"{self.base_url}/esearch.fcgi", params=params, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            id_list = result.get('esearchresult', {}).get('idlist', [])
            
            return id_list
            
        except Exception as e:
            logger.error(f"æœç´¢{db}å¤±è´¥: {str(e)}")
            return []
    
    def _fetch_sra_details(self, sra_id: str) -> Optional[Dict]:
        """è·å–SRAè¯¦æƒ…å¹¶æå–dbGaPä¿¡æ¯"""
        try:
            params = {
                'db': 'sra',
                'id': sra_id,
                'retmode': 'xml',
                'email': self.email
            }
            
            if self.api_key:
                params['api_key'] = self.api_key
            
            response = requests.get(f"{self.base_url}/efetch.fcgi", params=params, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            
            # æå–å…³é”®ä¿¡æ¯
            study_info = {
                'sra_id': sra_id,
                'source_strategy': 'sra_linked',
                'dbgap_accession': None,
                'bioproject': None,
                'biosample': None,
                'title': '',
                'organism': '',
                'strategy': '',
                'platform': '',
                'raw_xml': ET.tostring(root, encoding='unicode')[:5000]  # ä¿ç•™éƒ¨åˆ†XML
            }
            
            # æå–æ ‡é¢˜
            title_elem = root.find('.//TITLE')
            if title_elem is not None:
                study_info['title'] = title_elem.text
            
            # æå–BioProject
            bioproject_elem = root.find('.//EXTERNAL_ID[@namespace="BioProject"]')
            if bioproject_elem is not None:
                study_info['bioproject'] = bioproject_elem.text
            
            # æå–dbGaP accession (é€šå¸¸åœ¨XREF_LINKä¸­)
            for xref in root.findall('.//XREF_LINK'):
                db_elem = xref.find('DB')
                id_elem = xref.find('ID')
                if db_elem is not None and id_elem is not None:
                    if db_elem.text == 'dbGaP':
                        study_info['dbgap_accession'] = id_elem.text
            
            # ä»æè¿°ä¸­æå–
            if not study_info['dbgap_accession']:
                description = root.find('.//STUDY_DESCRIPTION')
                if description is not None and description.text:
                    accessions = re.findall(r'phs\d{6}\.v\d+\.p\d+', description.text)
                    if accessions:
                        study_info['dbgap_accession'] = accessions[0]
            
            # æå–æµ‹åºç­–ç•¥
            strategy_elem = root.find('.//LIBRARY_STRATEGY')
            if strategy_elem is not None:
                study_info['strategy'] = strategy_elem.text
            
            # æå–å¹³å°
            platform_elem = root.find('.//PLATFORM')
            if platform_elem is not None:
                for child in platform_elem:
                    study_info['platform'] = child.tag
                    break
            
            return study_info
            
        except Exception as e:
            logger.error(f"è·å–SRA {sra_id} è¯¦æƒ…å¤±è´¥: {str(e)}")
            return None
    
    def _fetch_bioproject_details(self, project_id: str) -> Optional[Dict]:
        """è·å–BioProjectè¯¦æƒ…"""
        try:
            params = {
                'db': 'bioproject',
                'id': project_id,
                'retmode': 'xml',
                'email': self.email
            }
            
            if self.api_key:
                params['api_key'] = self.api_key
            
            response = requests.get(f"{self.base_url}/efetch.fcgi", params=params, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            
            study_info = {
                'bioproject_id': project_id,
                'source_strategy': 'bioproject',
                'dbgap_accession': None,
                'title': '',
                'description': '',
                'organism': '',
                'data_type': []
            }
            
            # æå–é¡¹ç›®ä¿¡æ¯
            project = root.find('.//Project')
            if project is not None:
                # æ ‡é¢˜
                title_elem = project.find('.//ProjectDescr/Title')
                if title_elem is not None:
                    study_info['title'] = title_elem.text
                
                # æè¿°
                desc_elem = project.find('.//ProjectDescr/Description')
                if desc_elem is not None:
                    study_info['description'] = desc_elem.text
                
                # æå–dbGaPé“¾æ¥
                for ext_link in project.findall('.//ExternalLink'):
                    dbname = ext_link.find('dbname')
                    if dbname is not None and dbname.text == 'dbGaP':
                        label = ext_link.find('label')
                        if label is not None:
                            # ä»labelæå–accession
                            accessions = re.findall(r'phs\d{6}', label.text)
                            if accessions:
                                study_info['dbgap_accession'] = accessions[0]
                
                # ä»æè¿°ä¸­æå–
                if not study_info['dbgap_accession'] and study_info['description']:
                    accessions = re.findall(r'phs\d{6}\.v\d+\.p\d+', study_info['description'])
                    if accessions:
                        study_info['dbgap_accession'] = accessions[0]
            
            return study_info
            
        except Exception as e:
            logger.error(f"è·å–BioProject {project_id} è¯¦æƒ…å¤±è´¥: {str(e)}")
            return None
    
    def _parse_dbgap_json_study(self, study_data: Dict) -> Dict:
        """è§£ædbGaP JSONæ ¼å¼çš„ç ”ç©¶æ•°æ®"""
        return {
            'dbgap_accession': study_data.get('study_accession', ''),
            'source_strategy': 'dbgap_api',
            'title': study_data.get('study_name', ''),
            'disease': study_data.get('disease', ''),
            'study_type': study_data.get('study_type', ''),
            'participants': study_data.get('number_of_participants', ''),
            'registration_date': study_data.get('registration_date', ''),
            'raw_data': study_data
        }
    
    def _fetch_pubmed_details(self, pmid: str) -> Dict:
        """è·å–PubMedæ–‡çŒ®è¯¦æƒ…"""
        try:
            params = {
                'db': 'pubmed',
                'id': pmid,
                'retmode': 'xml',
                'email': self.email
            }
            
            if self.api_key:
                params['api_key'] = self.api_key
            
            response = requests.get(f"{self.base_url}/efetch.fcgi", params=params, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            
            article = root.find('.//Article')
            if article is None:
                return {}
            
            pub_info = {
                'pmid': pmid,
                'title': self._get_xml_text(article, './/ArticleTitle'),
                'journal': self._get_xml_text(article, './/Journal/Title'),
                'pub_date': self._get_xml_text(article, './/PubDate/Year'),
                'abstract': self._get_xml_text(article, './/Abstract/AbstractText'),
                'raw_xml': ET.tostring(article, encoding='unicode')[:5000]
            }
            
            return pub_info
            
        except Exception as e:
            logger.error(f"è·å–PMID {pmid} å¤±è´¥: {str(e)}")
            return {}
    
    def _extract_dbgap_accessions(self, pub_info: Dict) -> List[str]:
        """ä»æ–‡çŒ®ä¿¡æ¯ä¸­æå–dbGaP accessionå·"""
        accessions = []
        
        text = f"{pub_info.get('title', '')} {pub_info.get('abstract', '')} {pub_info.get('raw_xml', '')}"
        
        # åŒ¹é… phs000000.v1.p1 æ ¼å¼
        matches = re.findall(r'phs\d{6}\.v\d+\.p\d+', text)
        accessions.extend(matches)
        
        # åŒ¹é…ç®€åŒ–æ ¼å¼ phs000000
        matches = re.findall(r'phs\d{6}', text)
        accessions.extend(matches)
        
        return list(set(accessions))
    
    def _get_xml_text(self, parent: ET.Element, xpath: str) -> str:
        """ä»XMLå…ƒç´ ä¸­æå–æ–‡æœ¬"""
        elem = parent.find(xpath)
        return elem.text if elem is not None and elem.text else ""
    
    def enrich_dbgap_studies(self, studies: List[Dict]) -> List[Dict]:
        """
        ä¸°å¯ŒdbGaPç ”ç©¶ä¿¡æ¯
        é€šè¿‡accessionå·è·å–å®Œæ•´çš„ç ”ç©¶è¯¦æƒ…
        """
        logger.info("=" * 70)
        logger.info(f"å¼€å§‹ä¸°å¯Œ {len(studies)} ä¸ªç ”ç©¶çš„è¯¦ç»†ä¿¡æ¯...")
        logger.info("=" * 70)
        
        enriched_studies = []
        unique_accessions = {}
        
        # å»é‡
        for study in studies:
            acc = study.get('dbgap_accession')
            if acc and acc not in unique_accessions:
                unique_accessions[acc] = study
        
        logger.info(f"å»é‡åå‰©ä½™ {len(unique_accessions)} ä¸ªå”¯ä¸€ç ”ç©¶")
        
        for idx, (accession, study) in enumerate(unique_accessions.items(), 1):
            logger.info(f"[{idx}/{len(unique_accessions)}] ä¸°å¯Œç ”ç©¶: {accession}")
            
            try:
                # é€šè¿‡dbGaPç½‘é¡µè·å–è¯¦ç»†ä¿¡æ¯
                details = self._fetch_dbgap_web_details(accession)
                
                if details:
                    # åˆå¹¶ä¿¡æ¯
                    enriched = {**study, **details}
                    enriched_studies.append(enriched)
                    logger.info(f"  âœ“ æˆåŠŸè·å–è¯¦ç»†ä¿¡æ¯")
                else:
                    enriched_studies.append(study)
                    logger.info(f"  âš  ä½¿ç”¨åŸå§‹ä¿¡æ¯")
                
                time.sleep(self.request_delay)
                
            except Exception as e:
                logger.error(f"  âœ— å¤±è´¥: {str(e)}")
                enriched_studies.append(study)
        
        logger.info(f"âœ“ ä¿¡æ¯ä¸°å¯Œå®Œæˆ")
        self._save_json(enriched_studies, "enriched_studies.json")
        
        return enriched_studies
    
    def _fetch_dbgap_web_details(self, accession: str) -> Optional[Dict]:
        """
        ä»dbGaPç½‘é¡µè·å–ç ”ç©¶è¯¦æƒ…
        """
        # æ¸…ç†accessionï¼ˆå»é™¤ç‰ˆæœ¬å·ï¼‰
        base_accession = accession.split('.')[0]
        
        # dbGaP studyé¡µé¢URL
        url = f"https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id={base_accession}"
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            html = response.text
            
            details = {
                'web_url': url,
                'html_fetched': True
            }
            
            # ç®€å•çš„HTMLè§£ææå–å…³é”®ä¿¡æ¯ï¼ˆå¯ä»¥ä½¿ç”¨BeautifulSoupè¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
            # è¿™é‡Œæä¾›åŸºæœ¬çš„æ­£åˆ™æå–
            
            # æå–PIä¿¡æ¯
            pi_match = re.search(r'Principal Investigator.*?<td[^>]*>(.*?)</td>', html, re.DOTALL)
            if pi_match:
                details['pi'] = re.sub(r'<[^>]+>', '', pi_match.group(1)).strip()
            
            # æå–æœºæ„
            inst_match = re.search(r'Institution.*?<td[^>]*>(.*?)</td>', html, re.DOTALL)
            if inst_match:
                details['institution'] = re.sub(r'<[^>]+>', '', inst_match.group(1)).strip()
            
            return details
            
        except Exception as e:
            logger.error(f"è·å–{accession}ç½‘é¡µè¯¦æƒ…å¤±è´¥: {str(e)}")
            return None
    
    def create_integrated_tables(self, studies: List[Dict]) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        åˆ›å»ºæ•´åˆçš„serieså’Œsampleè¡¨
        """
        logger.info("=" * 70)
        logger.info("åˆ›å»ºæ•´åˆæ•°æ®è¡¨...")
        logger.info("=" * 70)
        
        series_records = []
        
        for study in studies:
            # è§£æç–¾ç—…ä¿¡æ¯
            disease_text = study.get('disease', '')
            disease_general = self._classify_disease(disease_text)
            
            # è·å–ç¬¬ä¸€ç¯‡ç›¸å…³æ–‡çŒ®
            pmid = study.get('source_pmid', '')
            if not pmid and study.get('publication'):
                pmid = study.get('publication', {}).get('pmid', '')
            
            series_record = {
                'id': study.get('dbgap_accession', ''),
                'title': study.get('title', '')[:200],
                'disease_general': disease_general,
                'disease': disease_text,
                'pubmed': pmid,
                'source_database': 'dbGaP',
                'access_link': study.get('web_url', f"https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id={study.get('dbgap_accession', '').split('.')[0]}"),
                'open_status': 'Controlled Access',
                'ethnicity': self._extract_field(study, ['ethnicity', 'race', 'population']),
                'sex': self._extract_field(study, ['sex', 'gender']),
                'tissue': self._extract_field(study, ['tissue', 'sample_type', 'biosample']),
                'sequencing_platform': study.get('platform', self._extract_platform(study)),
                'experiment_design': 'single-cell RNA-seq',
                'sample_type': self._determine_sample_type(study),
                'summary': study.get('description', '')[:500],
                'citation_count': '',
                'publication_date': study.get('publication', {}).get('pub_date', ''),
                'submission_date': study.get('registration_date', ''),
                'last_update_date': '',
                'contact_name': study.get('pi', ''),
                'contact_email': '',
                'contact_institute': study.get('institution', ''),
                'data_tier': 'raw',
                'supplementary_information': json.dumps({
                    'source_strategy': study.get('source_strategy', ''),
                    'bioproject': study.get('bioproject', ''),
                    'sra_id': study.get('sra_id', ''),
                    'participants': study.get('participants', '')
                }, ensure_ascii=False)
            }
            
            series_records.append(series_record)
        
        series_df = pd.DataFrame(series_records)
        
        # ä¿å­˜Seriesè¡¨
        series_file = self.output_dir / "series_table.tsv"
        series_df.to_csv(series_file, sep='\t', index=False)
        logger.info(f"âœ“ Seriesè¡¨å·²ä¿å­˜: {series_file}")
        logger.info(f"  åŒ…å« {len(series_df)} æ¡è®°å½•")
        
        # åˆ›å»ºæ ·æœ¬è¡¨éª¨æ¶ï¼ˆå®é™…æ ·æœ¬ä¿¡æ¯éœ€è¦æˆæƒè®¿é—®ï¼‰
        sample_df = self._create_sample_template(series_df)
        
        sample_file = self.output_dir / "sample_table_template.tsv"
        sample_df.to_csv(sample_file, sep='\t', index=False)
        logger.info(f"âœ“ æ ·æœ¬è¡¨æ¨¡æ¿å·²ä¿å­˜: {sample_file}")
        
        return series_df, sample_df
    
    def _classify_disease(self, disease_text: str) -> str:
        """ç–¾ç—…åˆ†ç±»"""
        if not disease_text:
            return 'unknown'
        
        disease_lower = disease_text.lower()
        
        categories = {
            'cancer': ['cancer', 'tumor', 'carcinoma', 'leukemia', 'lymphoma', 'melanoma'],
            'neurodegenerative': ['alzheimer', 'parkinson', 'dementia', 'als', 'huntington'],
            'autoimmune': ['lupus', 'arthritis', 'diabetes', 'sclerosis', 'crohn'],
            'cardiovascular': ['heart', 'cardiovascular', 'coronary', 'cardiac'],
            'infectious': ['covid', 'virus', 'infection', 'bacterial', 'sepsis'],
            'metabolic': ['diabetes', 'obesity', 'metabolic'],
            'respiratory': ['asthma', 'copd', 'lung', 'pulmonary']
        }
        
        for category, keywords in categories.items():
            if any(kw in disease_lower for kw in keywords):
                return category
        
        return 'other'
    
    def _extract_field(self, study: Dict, field_names: List[str]) -> str:
        """ä»ç ”ç©¶æ•°æ®ä¸­æå–å­—æ®µ"""
        for field in field_names:
            if field in study and study[field]:
                return str(study[field])
        
        # ä»æè¿°ä¸­æå–
        text = f"{study.get('title', '')} {study.get('description', '')}".lower()
        
        # æ ¹æ®å­—æ®µç±»å‹è¿›è¡Œæ¨¡å¼åŒ¹é…
        if 'ethnicity' in field_names or 'race' in field_names:
            patterns = ['european', 'asian', 'african', 'hispanic', 'caucasian']
            for pattern in patterns:
                if pattern in text:
                    return pattern
        
        return 'unknown'
    
    def _extract_platform(self, study: Dict) -> str:
        """æå–æµ‹åºå¹³å°"""
        text = f"{study.get('title', '')} {study.get('description', '')} {study.get('platform', '')}".lower()
        
        platforms = {
            '10x Genomics': ['10x', 'chromium', '10x genomics'],
            'Illumina': ['illumina', 'nextseq', 'hiseq', 'novaseq'],
            'Smart-seq2': ['smart-seq', 'smartseq'],
            'Drop-seq': ['drop-seq', 'dropseq'],
            'inDrop': ['indrop']
        }
        
        for platform, keywords in platforms.items():
            if any(kw in text for kw in keywords):
                return platform
        
        return 'unknown'
    
    def _determine_sample_type(self, study: Dict) -> str:
        """åˆ¤æ–­æ ·æœ¬ç±»å‹"""
        text = f"{study.get('title', '')} {study.get('disease', '')}".lower()
        
        if any(word in text for word in ['healthy', 'normal', 'control']):
            if any(word in text for word in ['disease', 'patient', 'tumor', 'cancer']):
                return 'mixed'
            return 'healthy'
        elif any(word in text for word in ['disease', 'patient', 'tumor', 'cancer']):
            return 'disease'
        
        return 'unknown'
    
    def _create_sample_template(self, series_df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ ·æœ¬è¡¨æ¨¡æ¿"""
        sample_records = []
        
        for _, row in series_df.iterrows():
            # ä¸ºæ¯ä¸ªç ”ç©¶åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ ·æœ¬è®°å½•
            sample_record = {
                'id': f"{row['id']}_sample_template",
                'sample_id': 'Requires authorized access',
                'title': row['title'],
                'disease_general': row['disease_general'],
                'disease': row['disease'],
                'ethnicity': row['ethnicity'],
                'sex': row['sex'],
                'tissue_location': row['tissue'],
                'sequencing_platform': row['sequencing_platform'],
                'experiment_design': row['experiment_design'],
                'sample_type': row['sample_type']
            }
            
            sample_records.append(sample_record)
        
        return pd.DataFrame(sample_records)
    
    def _save_json(self, data: any, filename: str):
        """ä¿å­˜JSONæ–‡ä»¶"""
        filepath = self.output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        logger.info(f"  ğŸ’¾ å·²ä¿å­˜: {filepath}")
    
    def generate_summary_report(self, series_df: pd.DataFrame):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        logger.info("=" * 70)
        logger.info("ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
        logger.info("=" * 70)
        
        report = {
            'generation_time': datetime.now().isoformat(),
            'total_studies': len(series_df),
            'statistics': {
                'disease_distribution': series_df['disease_general'].value_counts().to_dict(),
                'platform_distribution': series_df['sequencing_platform'].value_counts().to_dict(),
                'sample_type_distribution': series_df['sample_type'].value_counts().to_dict(),
                'access_status': series_df['open_status'].value_counts().to_dict()
            },
            'data_sources': {
                'databases': ['dbGaP', 'SRA', 'BioProject', 'PubMed'],
                'strategies_used': [
                    'SRA-linked search',
                    'BioProject search',
                    'dbGaP API',
                    'PubMed literature mining'
                ]
            }
        }
        
        report_file = self.output_dir / "summary_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  æ€»ç ”ç©¶æ•°: {report['total_studies']}")
        logger.info(f"  ç–¾ç—…åˆ†å¸ƒ: {report['statistics']['disease_distribution']}")
        logger.info(f"  å¹³å°åˆ†å¸ƒ: {report['statistics']['platform_distribution']}")
        logger.info("=" * 70)


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç­–ç•¥"""
    print("=" * 80)
    print(" " * 15 + "dbGaP å•ç»†èƒæµ‹åºæ•°æ®æ£€ç´¢å·¥å…· v2.0")
    print(" " * 20 + "å¤šç­–ç•¥æ•´åˆç‰ˆ - 2025-12-02")
    print("=" * 80)
    print()
    
    # è·å–ç”¨æˆ·é‚®ç®±
    email = input("è¯·è¾“å…¥æ‚¨çš„é‚®ç®±åœ°å€ï¼ˆNCBIè¦æ±‚ï¼‰: ").strip()
    if not email:
        email = "user@example.com"
        print(f"ä½¿ç”¨é»˜è®¤é‚®ç®±: {email}")
    
    api_key = input("å¦‚æœ‰NCBI API keyè¯·è¾“å…¥ï¼ˆå¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡ï¼‰: ").strip()
    
    print()
    
    # åˆå§‹åŒ–çˆ¬å–å™¨
    scraper = dbGaPScraperV2(output_dir="dbgap_metadata_v2", email=email)
    if api_key:
        scraper.api_key = api_key
        scraper.request_delay = 0.1
        logger.info("å·²è®¾ç½®API keyï¼Œä½¿ç”¨æ›´å¿«çš„è¯·æ±‚é€Ÿç‡")
    
    all_studies = []
    
    # è¿è¡Œç­–ç•¥1: SRAæœç´¢
    print("\n" + "ğŸ” è¿è¡Œç­–ç•¥1: é€šè¿‡SRAæœç´¢...")
    try:
        studies_1 = scraper.strategy_1_search_sra_for_dbgap()
        all_studies.extend(studies_1)
        print(f"âœ… ç­–ç•¥1å®Œæˆï¼Œæ‰¾åˆ° {len(studies_1)} ä¸ªç ”ç©¶\n")
    except Exception as e:
        print(f"âŒ ç­–ç•¥1å¤±è´¥: {str(e)}\n")
    
    # è¿è¡Œç­–ç•¥2: BioProjectæœç´¢
    print("ğŸ” è¿è¡Œç­–ç•¥2: é€šè¿‡BioProjectæœç´¢...")
    try:
        studies_2 = scraper.strategy_2_search_bioproject()
        all_studies.extend(studies_2)
        print(f"âœ… ç­–ç•¥2å®Œæˆï¼Œæ‰¾åˆ° {len(studies_2)} ä¸ªç ”ç©¶\n")
    except Exception as e:
        print(f"âŒ ç­–ç•¥2å¤±è´¥: {str(e)}\n")
    
    # è¿è¡Œç­–ç•¥3: dbGaP API
    print("ğŸ” è¿è¡Œç­–ç•¥3: è®¿é—®dbGaP API...")
    try:
        studies_3 = scraper.strategy_3_direct_dbgap_portal()
        all_studies.extend(studies_3)
        print(f"âœ… ç­–ç•¥3å®Œæˆï¼Œæ‰¾åˆ° {len(studies_3)} ä¸ªç ”ç©¶\n")
    except Exception as e:
        print(f"âŒ ç­–ç•¥3å¤±è´¥: {str(e)}\n")
    
    # è¿è¡Œç­–ç•¥4: PubMedæœç´¢
    print("ğŸ” è¿è¡Œç­–ç•¥4: é€šè¿‡PubMedæ–‡çŒ®æœç´¢...")
    try:
        studies_4 = scraper.strategy_4_pubmed_linked_search()
        all_studies.extend(studies_4)
        print(f"âœ… ç­–ç•¥4å®Œæˆï¼Œæ‰¾åˆ° {len(studies_4)} ä¸ªç ”ç©¶\n")
    except Exception as e:
        print(f"âŒ ç­–ç•¥4å¤±è´¥: {str(e)}\n")
    
    if not all_studies:
        print("âŒ æ‰€æœ‰ç­–ç•¥å‡æœªæ‰¾åˆ°ç ”ç©¶ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"\nâœ… åˆè®¡æ‰¾åˆ° {len(all_studies)} ä¸ªç ”ç©¶è®°å½•ï¼ˆåŒ…å«é‡å¤ï¼‰\n")
    
    # ä¸°å¯Œç ”ç©¶ä¿¡æ¯
    print("ğŸ“ ä¸°å¯Œç ”ç©¶è¯¦ç»†ä¿¡æ¯...")
    enriched_studies = scraper.enrich_dbgap_studies(all_studies)
    print(f"âœ… ä¿¡æ¯ä¸°å¯Œå®Œæˆ\n")
    
    # åˆ›å»ºæ•°æ®è¡¨
    print("ğŸ“Š åˆ›å»ºæ ‡å‡†åŒ–æ•°æ®è¡¨...")
    series_df, sample_df = scraper.create_integrated_tables(enriched_studies)
    print(f"âœ… æ•°æ®è¡¨åˆ›å»ºå®Œæˆ\n")
    
    # ç”ŸæˆæŠ¥å‘Š
    scraper.generate_summary_report(series_df)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {scraper.output_dir.absolute()}")
    print("=" * 80)
    
    print("\nğŸ“‹ è¾“å‡ºæ–‡ä»¶æ¸…å•:")
    print("  ç­–ç•¥ç»“æœ:")
    print("    â”œâ”€ strategy1_results.json      (SRAæœç´¢ç»“æœ)")
    print("    â”œâ”€ strategy2_results.json      (BioProjectæœç´¢ç»“æœ)")
    print("    â”œâ”€ strategy3_results.json      (dbGaP APIç»“æœ)")
    print("    â””â”€ strategy4_results.json      (PubMedæœç´¢ç»“æœ)")
    print("  æ•´åˆæ•°æ®:")
    print("    â”œâ”€ enriched_studies.json       (ä¸°å¯Œåçš„ç ”ç©¶ä¿¡æ¯)")
    print("    â”œâ”€ series_table.tsv            (Seriesæ•°æ®è¡¨)")
    print("    â”œâ”€ sample_table_template.tsv   (æ ·æœ¬è¡¨æ¨¡æ¿)")
    print("    â”œâ”€ summary_report.json         (æ±‡æ€»æŠ¥å‘Š)")
    print("    â””â”€ dbgap_scraper_v2.log        (è¿è¡Œæ—¥å¿—)")
    print()
    
    print("âš ï¸  é‡è¦æç¤º:")
    print("   1. æ ·æœ¬çº§åˆ«è¯¦ç»†æ•°æ®éœ€è¦ç”³è¯·dbGaPæˆæƒè®¿é—®")
    print("   2. è®¿é—® https://dbgap.ncbi.nlm.nih.gov ç”³è¯·DAR")
    print("   3. ä½¿ç”¨ sra-toolkit é…åˆæˆæƒä¸‹è½½åŸå§‹æ•°æ®")
    print("   4. å½“å‰è¡¨æ ¼æä¾›ç ”ç©¶çº§åˆ«çš„å…ƒæ•°æ®æ¦‚è§ˆ")
    print()


if __name__ == "__main__":
    main()